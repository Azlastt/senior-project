{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3501276,"sourceType":"datasetVersion","datasetId":2107243}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-28T15:38:33.362330Z","iopub.execute_input":"2024-08-28T15:38:33.362673Z","iopub.status.idle":"2024-08-28T15:38:36.994776Z","shell.execute_reply.started":"2024-08-28T15:38:33.362645Z","shell.execute_reply":"2024-08-28T15:38:36.993729Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\n\n# Paths to the dataset directories\ntrain_dir = '/kaggle/input/irmas-training-data/IRMAS-TrainingData'\n# test_dir = 'path_to_IRMAS/test/'\n\n# Function to load and preprocess the audio files\ndef load_audio_files(directory):\n    X = []\n    y = []\n    labels = [d for d in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, d))]\n    label_dict = {label: i for i, label in enumerate(labels)}\n\n    for label in labels:\n        instrument_dir = os.path.join(directory, label)\n        for file_name in os.listdir(instrument_dir):\n            if file_name.endswith('.wav'):\n                file_path = os.path.join(instrument_dir, file_name)\n                # Load audio file\n                y_audio, sr = librosa.load(file_path, sr=None)\n                \n                # Convert to Mel-spectrogram\n                mel_spect = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=128, fmax=8000)\n                mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n                \n                # Compute RMS energy\n                rms = librosa.feature.rms(y=y_audio)[0]  # [0] to get the first (and only) row of RMS values\n                \n                # Reshape and pad/truncate to match the Mel-spectrogram dimensions\n                if mel_spect_db.shape[1] < 128:\n                    mel_spect_db = np.pad(mel_spect_db, ((0, 0), (0, 128 - mel_spect_db.shape[1])), mode='constant')\n                    rms = np.pad(rms, (0, 128 - len(rms)), mode='constant')\n                else:\n                    mel_spect_db = mel_spect_db[:, :128]\n                    rms = rms[:128]\n                \n                # Normalize RMS to match spectrogram scale\n                rms = rms / np.max(rms)  # Normalize to [0, 1]\n                \n                # Add RMS as an additional channel\n                rms = rms[np.newaxis, :]  # Add a new axis to match dimensions (1, time)\n                \n                # Combine Mel-spectrogram and RMS into a multi-channel input\n                combined_features = np.vstack([mel_spect_db, rms])  # Stack along the frequency axis\n                \n                # Add channel dimension\n                combined_features = combined_features[..., np.newaxis]\n                \n                # Append to dataset\n                X.append(combined_features)\n                y.append(label_dict[label])\n\n    X = np.array(X)\n    y = to_categorical(y, num_classes=len(labels))\n    return X, y\n\n# Load training data\nX_train, y_train = load_audio_files(train_dir)\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T04:27:51.917803Z","iopub.execute_input":"2024-08-28T04:27:51.918198Z","iopub.status.idle":"2024-08-28T04:35:16.316754Z","shell.execute_reply.started":"2024-08-28T04:27:51.918165Z","shell.execute_reply":"2024-08-28T04:35:16.314904Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-28 04:27:54.354269: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-28 04:27:54.354413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-28 04:27:54.524789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Training data shape: (6705, 129, 128, 1)\nTraining labels shape: (6705, 11)\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(X_train[:5])\nlabels = [d for d in sorted(os.listdir(train_dir)) if os.path.isdir(os.path.join(train_dir, d))]\nlabel_dict = {label: i for i, label in enumerate(labels)}\nprint(label_dict)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Convert one-hot encoded labels back to integers\ny_train_labels = np.argmax(y_train, axis=1)\ny_train_label = {'cel': 0, 'cla': 1, 'flu': 2, 'gac': 3, 'gel': 4, 'org': 5, 'pia': 6, 'sax': 7, 'tru': 8, 'vio': 9, 'voi': 10}\n\n\n# Plotting the distribution\nplt.figure(figsize=(10, 6))\nsns.countplot(x=y_train_labels)\nplt.title('Distribution of Classes in y_train')\nplt.xlabel('Class Labels')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:17:26.590890Z","iopub.execute_input":"2024-08-27T07:17:26.591269Z","iopub.status.idle":"2024-08-27T07:17:27.125825Z","shell.execute_reply.started":"2024-08-27T07:17:26.591239Z","shell.execute_reply":"2024-08-27T07:17:27.124596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train[0].shape)\nprint(X_train.min())\nprint(X_train.max())\n\nprint(X_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:17:30.795281Z","iopub.execute_input":"2024-08-27T07:17:30.796093Z","iopub.status.idle":"2024-08-27T07:17:30.891411Z","shell.execute_reply.started":"2024-08-27T07:17:30.796052Z","shell.execute_reply":"2024-08-27T07:17:30.890136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Normalize Mel-spectrogram values to [0, 1]\nX_train_normalized = (X_train + 80) / 80  # Assuming -80 dB as the min and 0 dB as the max\nprint(f\"Normalized range: {X_train_normalized.min()} to {X_train_normalized.max()}\")\n\n\n# Split the data into train and validation sets with stratification\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    X_train_normalized, y_train, test_size=0.1, stratify=y_train, random_state=42\n)\n\nprint(f\"Training data shape: {X_train_split.shape}\")\nprint(f\"Validation data shape: {X_val.shape}\")\nprint(f\"Training labels shape: {y_train_split.shape}\")\nprint(f\"Validation labels shape: {y_val.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:17:43.621495Z","iopub.execute_input":"2024-08-27T07:17:43.621922Z","iopub.status.idle":"2024-08-27T07:17:44.224833Z","shell.execute_reply.started":"2024-08-27T07:17:43.621891Z","shell.execute_reply":"2024-08-27T07:17:44.223786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport librosa.display\n\n# Function to display a sample waveform and its corresponding Mel-spectrogram\ndef display_waveform_and_spectrogram(y_audio, sr):\n    plt.figure(figsize=(14, 5))\n    \n    # Plot waveform\n    plt.subplot(1, 2, 1)\n    librosa.display.waveshow(y_audio, sr=sr)\n    plt.title('Waveform')\n    plt.xlabel('Time (samples)')\n    plt.ylabel('Amplitude');\n    \n    # Convert to Mel-spectrogram\n    mel_spect = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=128, fmax=8000)\n    mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n    \n    # Plot Mel-spectrogram\n    plt.subplot(1, 2, 2)\n    librosa.display.specshow(mel_spect_db, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n    plt.title('Mel-Spectrogram')\n    plt.colorbar(format='%+2.0f dB')\n    plt.show()\n\n# List files in the 'cel' directory\ncel_files = os.listdir(os.path.join(train_dir, 'pia'))\n\n# Display for random sample\nimport random\n\n# Randomly select a file from the 'cel' directory\nrandom_file = random.choice(cel_files)\nprint(f\"Selected file: {random_file}\")\n\ny_audio, sr = librosa.load(os.path.join(train_dir, 'pia', random_file), sr=None)\ndisplay_waveform_and_spectrogram(y_audio, sr)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:20:54.473223Z","iopub.execute_input":"2024-08-27T07:20:54.474259Z","iopub.status.idle":"2024-08-27T07:20:55.512630Z","shell.execute_reply.started":"2024-08-27T07:20:54.474219Z","shell.execute_reply":"2024-08-27T07:20:55.511533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Display the first image in X_train\nimage = X_train[0].reshape(129, 128)  # Reshape to (128, 128) to remove the last dimension\n\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale images\nplt.title('First Image in X_train')\nplt.show()\n\n# Display multiple images in a grid\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\naxes = axes.ravel()\n\nfor i in range(10):\n    image = X_train[i*350].reshape(129, 128)  # Reshape each image\n    y_train_index = np.argmax(y_train[i*350])\n    axes[i].imshow(image, cmap='gray')\n    axes[i].set_title(f'Image {i+1}: {list(y_train_label.keys())[list(y_train_label.values()).index(y_train_index)]}')\n    axes[i].axis('off')  # Hide the axis\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:21:47.898195Z","iopub.execute_input":"2024-08-27T07:21:47.898632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simpliest model\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.AveragePooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.AveragePooling2D((2, 2)),\n    #layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=10, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)\n\n\n# Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(X_test, y_test)\n# print(f'Test Accuracy: {test_acc:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:21:54.389448Z","iopub.execute_input":"2024-08-27T07:21:54.390435Z","iopub.status.idle":"2024-08-27T07:23:36.339972Z","shell.execute_reply.started":"2024-08-27T07:21:54.390393Z","shell.execute_reply":"2024-08-27T07:23:36.338334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\n# Define the input shape based on your data\ninput_shape = X_train.shape[1:]  # For example, (129, 128, 1)\n\n# Build the CNN model\nmodel = models.Sequential()\n\n# First Convolutional Layer\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Second Convolutional Layer\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Third Convolutional Layer\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Flatten the output for the Dense layers\nmodel.add(layers.Flatten())\n\n# Fully connected layer\nmodel.add(layers.Dense(128, activation='relu'))\n\n# Output layer (assuming you have 10 classes)\nmodel.add(layers.Dense(11, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=10, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:24:14.969398Z","iopub.execute_input":"2024-08-27T07:24:14.969833Z","iopub.status.idle":"2024-08-27T07:42:16.328698Z","shell.execute_reply.started":"2024-08-27T07:24:14.969800Z","shell.execute_reply":"2024-08-27T07:42:16.326856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\n\n# Paths to the dataset directories\ntrain_dir = '/kaggle/input/irmas-training-data/IRMAS-TrainingData'\n\n# Function to load and preprocess the audio files\ndef load_audio_files(directory):\n    X_spectrogram = []\n    X_rms = []\n    y = []\n    labels = [d for d in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, d))]\n    label_dict = {label: i for i, label in enumerate(labels)}\n\n    for label in labels:\n        if label == 'voi':  # Skip the 'voi' label\n            continue\n        instrument_dir = os.path.join(directory, label)\n        for file_name in os.listdir(instrument_dir):\n            if file_name.endswith('.wav'):\n                file_path = os.path.join(instrument_dir, file_name)\n                # Load audio file\n                y_audio, sr = librosa.load(file_path, sr=None)\n                \n                # Convert to Mel-spectrogram\n                mel_spect = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=128, fmax=8000)\n                mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n                \n                # Compute RMS energy\n                rms = librosa.feature.rms(y=y_audio)[0]\n                \n                # Reshape and pad/truncate to match the Mel-spectrogram dimensions\n                if mel_spect_db.shape[1] < 128:\n                    mel_spect_db = np.pad(mel_spect_db, ((0, 0), (0, 128 - mel_spect_db.shape[1])), mode='constant')\n                    rms = np.pad(rms, (0, 128 - len(rms)), mode='constant')\n                else:\n                    mel_spect_db = mel_spect_db[:, :128]\n                    rms = rms[:128]\n                \n                # Add channel dimension\n                mel_spect_db = mel_spect_db[..., np.newaxis]\n                rms = rms[..., np.newaxis]\n                \n                # Append to datasets\n                X_spectrogram.append(mel_spect_db)\n                X_rms.append(rms)\n                y.append(label_dict[label])\n\n    X_spectrogram = np.array(X_spectrogram)\n    X_rms = np.array(X_rms)\n    y = to_categorical(y, num_classes=10)\n    return X_spectrogram, X_rms, y\n\n# Load training data\nX_spectrogram_train, X_rms_train, y_train = load_audio_files(train_dir)\n\nprint(f\"Spectrogram training data shape: {X_spectrogram_train.shape}\")\nprint(f\"RMS training data shape: {X_rms_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T08:57:31.712844Z","iopub.execute_input":"2024-08-27T08:57:31.713897Z","iopub.status.idle":"2024-08-27T09:01:55.832584Z","shell.execute_reply.started":"2024-08-27T08:57:31.713857Z","shell.execute_reply":"2024-08-27T09:01:55.831475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Normalize Mel-spectrogram values to [0, 1]\nX_spectrogram_train_normalized = (X_spectrogram_train + 80) / 80  # Assuming -80 dB as the min and 0 dB as the max\nprint(f\"Normalized range: {X_spectrogram_train_normalized.min()} to {X_spectrogram_train_normalized.max()}\")\nprint(f\"Normalized range: {X_rms_train.min()} to {X_rms_train.max()}\")\n\n\n# Split into training and test sets\nX_spectrogram_train, X_spectrogram_test, X_rms_train, X_rms_test, y_train, y_test = train_test_split(\n    X_spectrogram_train, X_rms_train, y_train, stratify=y_train, test_size=0.1, random_state=82)\n\n# Further split the training set into training and validation sets\nX_spectrogram_train, X_spectrogram_val, X_rms_train, X_rms_val, y_train, y_val = train_test_split(\n    X_spectrogram_train, X_rms_train, y_train, stratify=y_train, test_size=0.1, random_state=82)  \n\nprint(f\"Training data shape: {X_spectrogram_train.shape}\")\nprint(f\"Validation data shape: {X_spectrogram_val.shape}\")\nprint(f\"Test data shape: {X_spectrogram_test.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Validation labels shape: {y_val.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T09:02:09.330425Z","iopub.execute_input":"2024-08-27T09:02:09.331009Z","iopub.status.idle":"2024-08-27T09:02:09.750730Z","shell.execute_reply.started":"2024-08-27T09:02:09.330969Z","shell.execute_reply":"2024-08-27T09:02:09.749641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, models, Input\n\n# Spectrogram input\nspectrogram_input = Input(shape=(128, 128, 1), name='spectrogram_input')\nx1 = layers.Conv2D(32, (3, 3), activation='relu')(spectrogram_input)\nx1 = layers.MaxPooling2D((2, 2))(x1)\n\n# MFCC input\nmfcc_input = Input(shape=(13, 128, 1), name='mfcc_input')\nx2 = layers.Conv2D(32, (3, 3), activation='relu')(mfcc_input)\nx2 = layers.MaxPooling2D((2, 2))(x2)\n\n# Concatenate features\ncombined = layers.concatenate([x1, x2])\n\n# Flatten and add dense layers\nx = layers.Flatten()(combined)\nx = layers.Dense(64, activation='relu')(x)\noutput = layers.Dense(10, activation='softmax')(x)\n\nmodel = models.Model(inputs=[spectrogram_input, mfcc_input], outputs=output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, models, Input\n\n# Spectrogram input\nspectrogram_input = Input(shape=(128, 128, 1), name='spectrogram_input')\nx1 = layers.Conv2D(32, (3, 3), activation='relu')(spectrogram_input)\nx1 = layers.MaxPooling2D((2, 2))(x1)\nx1 = layers.Conv2D(64, (3, 3), activation='relu')(x1)\nx1 = layers.MaxPooling2D((2, 2))(x1)\nx1 = layers.Conv2D(128, (3, 3), activation='relu')(x1)\nx1 = layers.MaxPooling2D((2, 2))(x1)\nx1 = layers.Flatten()(x1)\n\n# RMS input\nrms_input = Input(shape=(128, 1), name='rms_input')\nx2 = layers.Conv1D(16, 3, activation='relu')(rms_input)\nx2 = layers.MaxPooling1D(2)(x2)\nx2 = layers.Conv1D(32, 3, activation='relu')(x2)\nx2 = layers.MaxPooling1D(2)(x2)\nx2 = layers.Flatten()(x2)\n\n# Combine the processed features\ncombined = layers.concatenate([x1, x2])\n\n# Flatten and add dense layers\nx = layers.Dense(128, activation='relu')(combined)\noutput = layers.Dense(10, activation='softmax')(x)  # Assuming 10 instrument classes\n\n# Define the model\nmodel = models.Model(inputs=[spectrogram_input, rms_input], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit([X_spectrogram_train, X_rms_train], y_train, \n          validation_data=([X_spectrogram_val, X_rms_val], y_val), \n          batch_size=64, \n          epochs=20)\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate([X_spectrogram_test, X_rms_test], y_test)\nprint(f\"Test accuracy: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T09:02:15.166983Z","iopub.execute_input":"2024-08-27T09:02:15.167734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the label dictionary\ny_train_label = {0: 'cel', 1: 'cla', 2: 'flu', 3: 'gac', 4: 'gel', 5: 'org', 6: 'pia', 7: 'sax', 8: 'tru', 9: 'vio', 10: 'voi'}\n\n# Predict on the validation set\ny_pred = model.predict([X_spectrogram_test, X_rms_test])\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\n# Create a list of label names based on the dictionary\nlabels = [y_train_label[i] for i in range(len(y_train_label))]\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T08:28:39.208238Z","iopub.execute_input":"2024-08-27T08:28:39.208796Z","iopub.status.idle":"2024-08-27T08:28:41.289261Z","shell.execute_reply.started":"2024-08-27T08:28:39.208759Z","shell.execute_reply":"2024-08-27T08:28:41.288135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a figure with two subplots (1 row, 2 columns)\nplt.figure(figsize=(10, 4))\n\n# Subplot 1: Loss\nplt.subplot(1, 2, 1)\nplt.title('Loss Value')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nprint('Loss:', history.history['loss'][-1])\nprint('Val_loss:', history.history['val_loss'][-1])\n\n# Subplot 2: Accuracy\nplt.subplot(1, 2, 2)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['accuracy', 'val_accuracy'])\nprint('Accuracy:', history.history['accuracy'][-1])\nprint('Val_accuracy:', history.history['val_accuracy'][-1])\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:06:38.964955Z","iopub.execute_input":"2024-08-14T12:06:38.965409Z","iopub.status.idle":"2024-08-14T12:06:39.641303Z","shell.execute_reply.started":"2024-08-14T12:06:38.965373Z","shell.execute_reply":"2024-08-14T12:06:39.639985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.AveragePooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.AveragePooling2D((2, 2)),\n    #layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=20, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)\n\n\n# Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(X_test, y_test)\n# print(f'Test Accuracy: {test_acc:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:20:25.231408Z","iopub.execute_input":"2024-08-14T12:20:25.231883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the label dictionary\ny_train_label = {0: 'cel', 1: 'cla', 2: 'flu', 3: 'gac', 4: 'gel', 5: 'org', 6: 'pia', 7: 'sax', 8: 'tru', 9: 'vio', 10: 'voi'}\n\n# Predict on the validation set\ny_pred = model.predict(X_val)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_val, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\n# Create a list of label names based on the dictionary\nlabels = [y_train_label[i] for i in range(len(y_train_label))]\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots (1 row, 2 columns)\nplt.figure(figsize=(10, 4))\n\n# Subplot 1: Loss\nplt.subplot(1, 2, 1)\nplt.title('Loss Value')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nprint('Loss:', history.history['loss'][-1])\nprint('Val_loss:', history.history['val_loss'][-1])\n\n# Subplot 2: Accuracy\nplt.subplot(1, 2, 2)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['accuracy', 'val_accuracy'])\nprint('Accuracy:', history.history['accuracy'][-1])\nprint('Val_accuracy:', history.history['val_accuracy'][-1])\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:41:23.185951Z","iopub.execute_input":"2024-08-14T10:41:23.186503Z","iopub.status.idle":"2024-08-14T10:41:27.041540Z","shell.execute_reply.started":"2024-08-14T10:41:23.186467Z","shell.execute_reply":"2024-08-14T10:41:27.037260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.5),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(11, activation='softmax')\n])\n\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=10, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)\n\n\n# Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(X_test, y_test)\n# print(f'Test Accuracy: {test_acc:.2f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the label dictionary\ny_train_label = {0: 'cel', 1: 'cla', 2: 'flu', 3: 'gac', 4: 'gel', 5: 'org', 6: 'pia', 7: 'sax', 8: 'tru', 9: 'vio', 10: 'voi'}\n\n# Predict on the validation set\ny_pred = model.predict(X_val)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_val, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\n# Create a list of label names based on the dictionary\nlabels = [y_train_label[i] for i in range(len(y_train_label))]\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots (1 row, 2 columns)\nplt.figure(figsize=(10, 4))\n\n# Subplot 1: Loss\nplt.subplot(1, 2, 1)\nplt.title('Loss Value')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nprint('Loss:', history.history['loss'][-1])\nprint('Val_loss:', history.history['val_loss'][-1])\n\n# Subplot 2: Accuracy\nplt.subplot(1, 2, 2)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['accuracy', 'val_accuracy'])\nprint('Accuracy:', history.history['accuracy'][-1])\nprint('Val_accuracy:', history.history['val_accuracy'][-1])\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.models import Model\n\n# Convert grayscale images to RGB format\ndef convert_to_rgb(X):\n    return np.repeat(X, 3, axis=-1)\n\n# Convert your data to RGB format\nX_train_rgb = convert_to_rgb(X_train_split)\nX_val_rgb = convert_to_rgb(X_val)\n\n# Load the VGG16 model with pre-trained weights\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n\n# Add custom layers\nx = base_model.output\nx = layers.Flatten()(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)  # Adding dropout for regularization\nx = layers.Dense(11, activation='softmax')(x)\n\n# Create the model\nmodel = Model(inputs=base_model.input, outputs=x)\n\n# Freeze the base model layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_rgb, y_train_split, \n                    epochs=10, \n                    batch_size=64, \n                    validation_data=(X_val_rgb, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)\n\n# Optional: Fine-tuning\n# Unfreeze some layers in base_model and continue training\nfor layer in base_model.layers[-4:]:  # Example: unfreeze the last 4 layers\n    layer.trainable = True\n\n# Recompile with a lower learning rate for fine-tuning\nmodel.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fine-tune the model\nhistory_fine = model.fit(X_train_rgb, y_train_split, \n                         epochs=5, \n                         batch_size=64, \n                         validation_data=(X_val_rgb, y_val), \n                         callbacks=[early_stopping],\n                         shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T10:41:46.899191Z","iopub.execute_input":"2024-08-14T10:41:46.899716Z","iopub.status.idle":"2024-08-14T10:42:10.031218Z","shell.execute_reply.started":"2024-08-14T10:41:46.899679Z","shell.execute_reply":"2024-08-14T10:42:10.028590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the label dictionary\ny_train_label = {0: 'cel', 1: 'cla', 2: 'flu', 3: 'gac', 4: 'gel', 5: 'org', 6: 'pia', 7: 'sax', 8: 'tru', 9: 'vio', 10: 'voi'}\n\n# Predict on the validation set\ny_pred = model.predict(X_val)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_val, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\n# Create a list of label names based on the dictionary\nlabels = [y_train_label[i] for i in range(len(y_train_label))]\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots (1 row, 2 columns)\nplt.figure(figsize=(10, 4))\n\n# Subplot 1: Loss\nplt.subplot(1, 2, 1)\nplt.title('Loss Value')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nprint('Loss:', history.history['loss'][-1])\nprint('Val_loss:', history.history['val_loss'][-1])\n\n# Subplot 2: Accuracy\nplt.subplot(1, 2, 2)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['accuracy', 'val_accuracy'])\nprint('Accuracy:', history.history['accuracy'][-1])\nprint('Val_accuracy:', history.history['val_accuracy'][-1])\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **end**","metadata":{}},{"cell_type":"code","source":"# FROM RESEARCH: https://www.sciencedirect.com/science/article/pii/S1877050922011966\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.35),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.45),\n    \n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Flatten(),\n    layers.Dropout(0.5),\n    \n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.75),\n    \n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=20, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T17:34:35.324507Z","iopub.execute_input":"2024-08-13T17:34:35.325658Z","iopub.status.idle":"2024-08-13T17:51:41.495874Z","shell.execute_reply.started":"2024-08-13T17:34:35.325615Z","shell.execute_reply":"2024-08-13T17:51:41.493929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Define the CNN model\nmodel = models.Sequential([\n    layers.Conv2D(128, (3, 3), activation='relu', input_shape=(128, 128, 1)),  # Adjust shape if necessary\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(8, activation='relu'),\n    layers.Flatten(),\n    layers.Dense(11, activation='softmax')  # Output layer with number of classes\n])\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Show model summary\nmodel.summary()\n\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 13/08 -- 1.1\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=20, \n                    #batch_size=256, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:28:35.290657Z","iopub.execute_input":"2024-08-12T17:28:35.291126Z","iopub.status.idle":"2024-08-12T17:38:47.834735Z","shell.execute_reply.started":"2024-08-12T17:28:35.291088Z","shell.execute_reply":"2024-08-12T17:38:47.833262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The lastest one\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D , Dropout\n\n\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n    keras.layers.Dropout(0.25),\n    keras.layers.Conv2D(64, (3, 3) , activation='relu'),\n    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n    keras.layers.Dropout(0.25),\n    keras.layers.Conv2D(128, (3, 3) , activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n    keras.layers.Dropout(0.25),\n    keras.layers.BatchNormalization(),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1024),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(11, activation='softmax')\n])\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics= ['accuracy'])\nbatch_size = 16\nepochs = 100\n\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=10, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T17:03:30.947418Z","iopub.execute_input":"2024-08-13T17:03:30.948096Z","iopub.status.idle":"2024-08-13T17:03:39.519240Z","shell.execute_reply.started":"2024-08-13T17:03:30.948043Z","shell.execute_reply":"2024-08-13T17:03:39.516791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 13/08 -2\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    #layers.Dropout(0.25),\n    layers.Dense(128),\n    # layers.Dropout(0.5),\n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=5, \n                    batch_size=128, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping],\n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:16:34.318385Z","iopub.execute_input":"2024-08-12T17:16:34.318834Z","iopub.status.idle":"2024-08-12T17:20:00.421516Z","shell.execute_reply.started":"2024-08-12T17:16:34.318798Z","shell.execute_reply":"2024-08-12T17:20:00.420202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Flatten(),\n    \n    layers.Dropout(0.5),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(11, activation='softmax')\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=20, \n                    validation_data=(X_val, y_val), \n                    callbacks=[early_stopping, reduce_lr],\n                    shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T16:33:43.876337Z","iopub.execute_input":"2024-08-12T16:33:43.876855Z","iopub.status.idle":"2024-08-12T16:35:57.810050Z","shell.execute_reply.started":"2024-08-12T16:33:43.876814Z","shell.execute_reply":"2024-08-12T16:35:57.807889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(128, 128, 1)),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    \n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    \n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    \n    GlobalAveragePooling2D(),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(11, activation='softmax')\n])\n\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_split, y_train_split, \n                    epochs=20, \n                    batch_size=64, \n                    validation_data=(X_val, y_val), \n                    shuffle=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n# Assuming X_train contains pre-extracted Mel-spectrograms\ndef normalize_features(features):\n    original_shape = features.shape\n    features = features.reshape(-1, original_shape[-1])\n    scaler = MinMaxScaler()\n    features_normalized = scaler.fit_transform(features)\n    return features_normalized.reshape(original_shape)\n\nX_train_normalized = normalize_features(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T08:20:21.222410Z","iopub.execute_input":"2024-08-06T08:20:21.222895Z","iopub.status.idle":"2024-08-06T08:20:21.857702Z","shell.execute_reply.started":"2024-08-06T08:20:21.222851Z","shell.execute_reply":"2024-08-06T08:20:21.856655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the CNN model\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),  # Adjust shape if necessary\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(11, activation='softmax')  # Output layer with number of classes\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train_normalized, y_train, epochs=20, batch_size=32, validation_split=0.2)\n\n# Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(X_test, y_test)\n# print(f'Test Accuracy: {test_acc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T08:21:10.630694Z","iopub.execute_input":"2024-08-06T08:21:10.631193Z","iopub.status.idle":"2024-08-06T08:57:55.271479Z","shell.execute_reply.started":"2024-08-06T08:21:10.631155Z","shell.execute_reply":"2024-08-06T08:57:55.270210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predict on the test set\ny_pred = model.predict(X_train_normalized)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_train, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T08:58:38.154791Z","iopub.execute_input":"2024-08-06T08:58:38.155250Z","iopub.status.idle":"2024-08-06T08:59:08.695405Z","shell.execute_reply.started":"2024-08-06T08:58:38.155212Z","shell.execute_reply":"2024-08-06T08:59:08.694257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='train accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Model Accuracy')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T09:01:57.241930Z","iopub.execute_input":"2024-08-06T09:01:57.242434Z","iopub.status.idle":"2024-08-06T09:01:57.538739Z","shell.execute_reply.started":"2024-08-06T09:01:57.242399Z","shell.execute_reply":"2024-08-06T09:01:57.537550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\n\ndef convert_to_rgb(images):\n    return np.concatenate([images] * 3, axis=-1)  # Repeat the grayscale channel 3 times\n\n# Convert X_train to RGB\nX_train_rgb = convert_to_rgb(X_train)\n\n# Update input shape to (128, 128, 3)\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n\n# Build and compile model\nmodel = models.Sequential([\n    base_model,\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(11, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train_rgb, y_train, epochs=20, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T09:02:27.093247Z","iopub.execute_input":"2024-08-06T09:02:27.093686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predict on the test set\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='train accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Model Accuracy')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}